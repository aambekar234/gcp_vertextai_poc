{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d9079023",
   "metadata": {},
   "source": [
    "# VERTEX AI POC\n",
    "\n",
    "1. Created new project in GCP.\n",
    "2. Enable Vertex AI APIs, Compute Engine APIs, and Cloud Storage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "c43190e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated property [core/project].\r\n"
     ]
    }
   ],
   "source": [
    "PROJECT_ID = \"homedepot-345802\"\n",
    "REGION = \"us-central1\"\n",
    "# set current project as project_id \n",
    "! gcloud config set project $PROJECT_ID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "262b89f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "\n",
    "TIMESTAMP = datetime.now().strftime(\"%Y%m%d%H%M%S\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030fdb7e",
   "metadata": {},
   "source": [
    "## Creating bucket to host data and model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "0a96090f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating gs://homedpot/...\r\n"
     ]
    }
   ],
   "source": [
    "# bucket we are going to use to host data and model\n",
    "BUCKET_NAME = \"gs://homedpot\"\n",
    "! gsutil mb -l $REGION $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "bbba0af9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      1493  2022-03-31T06:06:45Z  gs://homedpot/aiplatform-2022-03-31-02:06:45.075-aiplatform_custom_trainer_script-0.1.tar.gz#1648706805860842  metageneration=1\r\n",
      "      1501  2022-03-31T06:07:20Z  gs://homedpot/aiplatform-2022-03-31-02:07:20.208-aiplatform_custom_trainer_script-0.1.tar.gz#1648706840954735  metageneration=1\r\n",
      "      1888  2022-03-31T05:50:33Z  gs://homedpot/trainer_iris.tar.gz#1648705833240231  metageneration=1\r\n",
      "                                 gs://homedpot/20220330233852/\r\n",
      "TOTAL: 3 objects, 4882 bytes (4.77 KiB)\r\n"
     ]
    }
   ],
   "source": [
    "#validate if you have access\n",
    "! gsutil ls -al $BUCKET_NAME"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c5cd6d99",
   "metadata": {},
   "source": [
    "## Initializing AI Platform sdk "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d36aae46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:numexpr.utils:Note: NumExpr detected 10 cores but \"NUMEXPR_MAX_THREADS\" not set, so enforcing safe limit of 8.\n",
      "INFO:numexpr.utils:NumExpr defaulting to 8 threads.\n"
     ]
    }
   ],
   "source": [
    "import google.cloud.aiplatform as aip\n",
    "\n",
    "aip.init(project=PROJECT_ID, staging_bucket=BUCKET_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0bea3cb9",
   "metadata": {},
   "source": [
    "## Define containers for train and deploy and machine type for the same"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c3239b13",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_IMAGE = \"gcr.io/cloud-aiplatform/training/xgboost-cpu.1-1:latest\"\n",
    "DEPLOY_IMAGE = \"gcr.io/cloud-aiplatform/prediction/xgboost-cpu.1-1:latest\"\n",
    "TRAIN_COMPUTE = \"n1-standard-4\"\n",
    "DEPLOY_COMPUTE = \"n1-standard-4\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d43906f",
   "metadata": {},
   "source": [
    "## Setting up training package "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "5c374fff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make folder for Python training script\n",
    "! rm -rf custom\n",
    "! mkdir custom\n",
    "\n",
    "# Add package information\n",
    "! touch custom/README.md\n",
    "\n",
    "setup_cfg = \"[egg_info]\\n\\ntag_build =\\n\\ntag_date = 0\"\n",
    "! echo \"$setup_cfg\" > custom/setup.cfg\n",
    "\n",
    "setup_py = \"import setuptools\\n\\nsetuptools.setup(\\n\\n    install_requires=[\\n\\n        'tensorflow_datasets==1.3.0',\\n\\n    ],\\n\\n    packages=setuptools.find_packages())\"\n",
    "! echo \"$setup_py\" > custom/setup.py\n",
    "\n",
    "pkg_info = \"Metadata-Version: 1.0\\n\\nName: Iris tabular classification\\n\\nVersion: 0.0.0\\n\\nSummary: Demostration training script\\n\\nHome-page: www.google.com\\n\\nAuthor: Google\\n\\nAuthor-email: aferlitsch@google.com\\n\\nLicense: Public\\n\\nDescription: Demo\\n\\nPlatform: Vertex\"\n",
    "! echo \"$pkg_info\" > custom/PKG-INFO\n",
    "\n",
    "# Make the training subfolder\n",
    "! mkdir custom/trainer\n",
    "! touch custom/trainer/__init__.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "4dda668e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Writing custom/trainer/task.py\n"
     ]
    }
   ],
   "source": [
    "%%writefile custom/trainer/task.py\n",
    "# Single Instance Training for Iris\n",
    "\n",
    "import datetime\n",
    "import os\n",
    "import subprocess\n",
    "import sys\n",
    "import pandas as pd\n",
    "import xgboost as xgb\n",
    "\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model-dir', dest='model_dir',\n",
    "                    default=os.getenv('AIP_MODEL_DIR'), type=str, help='Model dir.')\n",
    "args = parser.parse_args()\n",
    "\n",
    "# Download data\n",
    "iris_data_filename = 'iris_data.csv'\n",
    "iris_target_filename = 'iris_target.csv'\n",
    "data_dir = 'gs://cloud-samples-data/ai-platform/iris'\n",
    "\n",
    "# gsutil outputs everything to stderr so we need to divert it to stdout.\n",
    "subprocess.check_call(['gsutil', 'cp', os.path.join(data_dir,\n",
    "                                                    iris_data_filename),\n",
    "                       iris_data_filename], stderr=sys.stdout)\n",
    "subprocess.check_call(['gsutil', 'cp', os.path.join(data_dir,\n",
    "                                                    iris_target_filename),\n",
    "                       iris_target_filename], stderr=sys.stdout)\n",
    "\n",
    "\n",
    "# Load data into pandas, then use `.values` to get NumPy arrays\n",
    "iris_data = pd.read_csv(iris_data_filename).values\n",
    "iris_target = pd.read_csv(iris_target_filename).values\n",
    "\n",
    "# Convert one-column 2D array into 1D array for use with XGBoost\n",
    "iris_target = iris_target.reshape((iris_target.size,))\n",
    "\n",
    "\n",
    "# Load data into DMatrix object\n",
    "dtrain = xgb.DMatrix(iris_data, label=iris_target)\n",
    "\n",
    "\n",
    "# Train XGBoost model\n",
    "bst = xgb.train({}, dtrain, 20)\n",
    "\n",
    "# Export the classifier to a file\n",
    "model_filename = 'model.bst'\n",
    "bst.save_model(model_filename)\n",
    "\n",
    "# Upload the saved model file to Cloud Storage\n",
    "gcs_model_path = os.path.join(args.model_dir, model_filename)\n",
    "subprocess.check_call(['gsutil', 'cp', model_filename, gcs_model_path],\n",
    "    stderr=sys.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "343d1061",
   "metadata": {},
   "source": [
    "## Upload training package to our gcp bucket"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "049a64c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "a custom\n",
      "a custom/PKG-INFO\n",
      "a custom/.DS_Store\n",
      "a custom/README.md\n",
      "a custom/setup.py\n",
      "a custom/setup.cfg\n",
      "a custom/trainer\n",
      "a custom/trainer/task.py\n",
      "a custom/trainer/__init__.py\n",
      "Copying file://custom.tar.gz [Content-Type=application/x-tar]...\n",
      "/ [1 files][  1.8 KiB/  1.8 KiB]                                                \n",
      "Operation completed over 1 objects/1.8 KiB.                                      \n"
     ]
    }
   ],
   "source": [
    "! rm -f custom.tar custom.tar.gz\n",
    "! tar cvf custom.tar custom\n",
    "! gzip custom.tar\n",
    "! gsutil cp custom.tar.gz $BUCKET_NAME/trainer_iris.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d6bb9b5e",
   "metadata": {},
   "source": [
    "## Create custom training job and run it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "ac950a04",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<google.cloud.aiplatform.training_jobs.CustomTrainingJob object at 0x7fcca3d50f50>\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "os.environ['GOOGLE_APPLICATION_CREDENTIALS'] = 'service_key.json'\n",
    "job = aip.CustomTrainingJob(\n",
    "    display_name=\"iris_\" + TIMESTAMP,\n",
    "    script_path=\"custom/trainer/task.py\",\n",
    "    container_uri=TRAIN_IMAGE,\n",
    "    requirements=[\"gcsfs==0.7.1\", \"tensorflow-datasets==4.4\"],\n",
    ")\n",
    "\n",
    "print(job)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "25243e1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.utils.source_utils:Training script copied to:\n",
      "gs://homedpot/aiplatform-2022-03-31-07:12:03.893-aiplatform_custom_trainer_script-0.1.tar.gz.\n",
      "INFO:google.cloud.aiplatform.training_jobs:Training Output directory:\n",
      "gs://homedpot/20220331071112 \n",
      "INFO:google.cloud.aiplatform.training_jobs:View Training:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/3420224981999550464?project=559280923802\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/559280923802/locations/us-central1/trainingPipelines/3420224981999550464 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:View backing custom job:\n",
      "https://console.cloud.google.com/ai/platform/locations/us-central1/training/5724537471027380224?project=559280923802\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/559280923802/locations/us-central1/trainingPipelines/3420224981999550464 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/559280923802/locations/us-central1/trainingPipelines/3420224981999550464 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob projects/559280923802/locations/us-central1/trainingPipelines/3420224981999550464 current state:\n",
      "PipelineState.PIPELINE_STATE_RUNNING\n",
      "INFO:google.cloud.aiplatform.training_jobs:CustomTrainingJob run completed. Resource name: projects/559280923802/locations/us-central1/trainingPipelines/3420224981999550464\n",
      "WARNING:google.cloud.aiplatform.training_jobs:Training did not produce a Managed Model returning None. Training Pipeline projects/559280923802/locations/us-central1/trainingPipelines/3420224981999550464 is not configured to upload a Model. Create the Training Pipeline with model_serving_container_image_uri and model_display_name passed in. Ensure that your training script saves to model to os.environ['AIP_MODEL_DIR'].\n"
     ]
    }
   ],
   "source": [
    "MODEL_DIR = \"{}/{}\".format(BUCKET_NAME, TIMESTAMP)\n",
    "\n",
    "\n",
    "job.run(\n",
    "    replica_count=1, machine_type=TRAIN_COMPUTE, base_output_dir=MODEL_DIR, sync=True\n",
    ")\n",
    "\n",
    "MODEL_DIR = MODEL_DIR + \"/model\"\n",
    "model_path_to_deploy = MODEL_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54339b37",
   "metadata": {},
   "source": [
    "## Upload model to model resource "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "2e978c17",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Model\n",
      "INFO:google.cloud.aiplatform.models:Create Model backing LRO: projects/559280923802/locations/us-central1/models/8252446094923923456/operations/2106227291666251776\n",
      "INFO:google.cloud.aiplatform.models:Model created. Resource name: projects/559280923802/locations/us-central1/models/8252446094923923456\n",
      "INFO:google.cloud.aiplatform.models:To use this Model in another session:\n",
      "INFO:google.cloud.aiplatform.models:model = aiplatform.Model('projects/559280923802/locations/us-central1/models/8252446094923923456')\n"
     ]
    }
   ],
   "source": [
    "model = aip.Model.upload(\n",
    "    display_name=\"iris_\" + TIMESTAMP,\n",
    "    artifact_uri=MODEL_DIR,\n",
    "    serving_container_image_uri=DEPLOY_IMAGE,\n",
    "    sync=False,\n",
    ")\n",
    "\n",
    "model.wait()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ac876218",
   "metadata": {},
   "source": [
    "## Deploy the model for online predictions. You can specify min/max nodes for scaling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "448eee4c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.models:Creating Endpoint\n",
      "INFO:google.cloud.aiplatform.models:Create Endpoint backing LRO: projects/559280923802/locations/us-central1/endpoints/2958979304391704576/operations/6717913310093639680\n",
      "INFO:google.cloud.aiplatform.models:Endpoint created. Resource name: projects/559280923802/locations/us-central1/endpoints/2958979304391704576\n",
      "INFO:google.cloud.aiplatform.models:To use this Endpoint in another session:\n",
      "INFO:google.cloud.aiplatform.models:endpoint = aiplatform.Endpoint('projects/559280923802/locations/us-central1/endpoints/2958979304391704576')\n",
      "INFO:google.cloud.aiplatform.models:Deploying model to Endpoint : projects/559280923802/locations/us-central1/endpoints/2958979304391704576\n",
      "INFO:google.cloud.aiplatform.models:Deploy Endpoint model backing LRO: projects/559280923802/locations/us-central1/endpoints/2958979304391704576/operations/3182587602607800320\n",
      "INFO:google.cloud.aiplatform.models:Endpoint model deployed. Resource name: projects/559280923802/locations/us-central1/endpoints/2958979304391704576\n"
     ]
    }
   ],
   "source": [
    "DEPLOY_COMPUTE = \"n1-standard-4\"\n",
    "DEPLOYED_NAME = \"iris-\" + TIMESTAMP\n",
    "\n",
    "TRAFFIC_SPLIT = {\"0\": 100}\n",
    "\n",
    "MIN_NODES = 1\n",
    "MAX_NODES = 1\n",
    "\n",
    "endpoint = model.deploy(\n",
    "    deployed_model_display_name=DEPLOYED_NAME,\n",
    "    traffic_split=TRAFFIC_SPLIT,\n",
    "    machine_type=DEPLOY_COMPUTE,\n",
    "    min_replica_count=MIN_NODES,\n",
    "    max_replica_count=MAX_NODES,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d5d8f42",
   "metadata": {},
   "source": [
    "## Get some predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "ac739fa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prediction(predictions=[2.045193195343018], deployed_model_id='440182883110354944', explanations=None)\n"
     ]
    }
   ],
   "source": [
    "instances_list = [[1.4, 1.3, 5.1, 2.8]]\n",
    "\n",
    "prediction = endpoint.predict(instances_list)\n",
    "print(prediction)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfe0724b",
   "metadata": {},
   "source": [
    "## Project Cleanup & Deleting all the resources on GCP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "ede14a91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "INFO:google.cloud.aiplatform.base:Deleting Model : projects/559280923802/locations/us-central1/models/8252446094923923456\n",
      "INFO:google.cloud.aiplatform.base:Delete Model  backing LRO: projects/559280923802/locations/us-central1/operations/1309090157621673984\n",
      "INFO:google.cloud.aiplatform.base:Model deleted. . Resource name: projects/559280923802/locations/us-central1/models/8252446094923923456\n",
      "INFO:google.cloud.aiplatform.base:Deleting Endpoint : projects/559280923802/locations/us-central1/endpoints/2958979304391704576\n",
      "INFO:google.cloud.aiplatform.base:Delete Endpoint  backing LRO: projects/559280923802/locations/us-central1/operations/5920776176049061888\n",
      "INFO:google.cloud.aiplatform.base:Endpoint deleted. . Resource name: projects/559280923802/locations/us-central1/endpoints/2958979304391704576\n",
      "INFO:google.cloud.aiplatform.base:Deleting CustomTrainingJob : projects/559280923802/locations/us-central1/trainingPipelines/3420224981999550464\n",
      "INFO:google.cloud.aiplatform.base:Delete CustomTrainingJob  backing LRO: projects/559280923802/locations/us-central1/operations/3614933166835367936\n",
      "INFO:google.cloud.aiplatform.base:CustomTrainingJob deleted. . Resource name: projects/559280923802/locations/us-central1/trainingPipelines/3420224981999550464\n",
      "Removing gs://homedpot/aiplatform-2022-03-31-02:06:45.075-aiplatform_custom_trainer_script-0.1.tar.gz#1648706805860842...\n",
      "Removing gs://homedpot/aiplatform-2022-03-31-02:07:20.208-aiplatform_custom_trainer_script-0.1.tar.gz#1648706840954735...\n",
      "Removing gs://homedpot/aiplatform-2022-03-31-07:12:03.893-aiplatform_custom_trainer_script-0.1.tar.gz#1648725124693195...\n",
      "Removing gs://homedpot/trainer_iris.tar.gz#1648705833240231...                  \n",
      "/ [4 objects]                                                                   \n",
      "==> NOTE: You are performing a sequence of gsutil operations that may\n",
      "run significantly faster if you instead use gsutil -m rm ... Please\n",
      "see the -m section under \"gsutil help options\" for further information\n",
      "about when gsutil -m can be advantageous.\n",
      "\n",
      "Removing gs://homedpot/20220330233852/model/model.bst#1648706993207983...\n",
      "Removing gs://homedpot/20220331071112/model/model.bst#1648725277475893...       \n",
      "/ [6 objects]                                                                   \n",
      "Operation completed over 6 objects.                                              \n",
      "Removing gs://homedpot/...\n"
     ]
    }
   ],
   "source": [
    "endpoint.undeploy_all()\n",
    "\n",
    "delete_all = True\n",
    "\n",
    "if delete_all:\n",
    "    # Delete the dataset using the Vertex dataset object\n",
    "    try:\n",
    "        if \"dataset\" in globals():\n",
    "            dataset.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the model using the Vertex model object\n",
    "    try:\n",
    "        if \"model\" in globals():\n",
    "            model.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the endpoint using the Vertex endpoint object\n",
    "    try:\n",
    "        if \"endpoint\" in globals():\n",
    "            endpoint.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the AutoML or Pipeline trainig job\n",
    "    try:\n",
    "        if \"dag\" in globals():\n",
    "            dag.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the custom trainig job\n",
    "    try:\n",
    "        if \"job\" in globals():\n",
    "            job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the batch prediction job using the Vertex batch prediction object\n",
    "    try:\n",
    "        if \"batch_predict_job\" in globals():\n",
    "            batch_predict_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    # Delete the hyperparameter tuning job using the Vertex hyperparameter tuning object\n",
    "    try:\n",
    "        if \"hpt_job\" in globals():\n",
    "            hpt_job.delete()\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "\n",
    "    if \"BUCKET_NAME\" in globals():\n",
    "        ! gsutil rm -r $BUCKET_NAME"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
